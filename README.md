# èœè°±å°ç®¡å®¶

## ä»‹ç»

ä½¿ç”¨é£Ÿè°±ç›¸å…³çš„æ•°æ®é›†å¯¹InternLMå¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®©å…¶å®ç°é—®å®ƒä¸€é“èœçš„åšæ³•ï¼Œå®ƒå¯ä»¥æ¸…æ™°åœ°å‘Šè¯‰å¤§å®¶æ€ä¹ˆåšã€‚

## OpenXlab æ¨¡å‹

&emsp;&emsp;èœè°±å°ç®¡å®¶ä½¿ç”¨çš„æ˜¯InternLM çš„ 7B æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°é‡ä¸º 7B

## æ•°æ®é›†

&emsp;&emsp;èœè°±å°ç®¡å®¶ æ•°æ®é›†é‡‡ç”¨çš„æ•°æ®ä¸ºOpenDataLabä¸Šçš„å…¬å¼€æ•°æ®é›†XiaChuFang Recipe Corpusï¼Œå…±è®¡150ç™¾ä¸‡ä½™æ¡ï¼Œåœ¨è®­ç»ƒé‡‡ç”¨äº†15ä¸‡æ¡æ•°æ®ã€‚

&emsp;&emsp;æ•°æ®é›†åœ°å€ï¼šhttps://opendatalab.com/OpenDataLab/XiaChuFang_Recipe_Corpus

&emsp;&emsp;æ•°æ®é›†æ ·ä¾‹ï¼š

```text
{
"system": "ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ä¸“ä¸šå¨å¸ˆï¼Œç”¨æˆ·æå‡ºè‡ªå·±æƒ³è¦åšçš„èœè‚´æ—¶ï¼Œä½ å¯ä»¥ç²¾ç¡®åœ°å›ç­”å‡ºåšèœçš„åŸæ–™å’Œåšèœçš„æ–¹æ³•",
"input": "è‡ªåˆ¶è…Œèœçš„åšæ³•",
"output": "æ‚¨éœ€è¦å‡†å¤‡ä»¥ä¸‹é£Ÿæ:\n1500å…‹ä¸Šæµ·é’\n45å…‹ç›\næŒ‰ä»¥ä¸‹æ–¹æ³•åˆ¶ä½œ:\nä¸Šæµ·é’ï¼ˆä¸ç”¨æ´—ï¼‰æ”¾é˜³å°ä¸Šæ™’å¹²è¡¨é¢çš„æ°´ä»½ï¼Œç„¶åæ”¾è¿›å¹²å‡€çš„ç›†å­é‡Œï¼Œä¸€ç‰‡ä¸€ç‰‡æ°å¼€ï¼ˆé›ªé‡Œè•»ä¸ç”¨æ°ï¼‰ï¼Œå†æ”¾ç›æ‰ï¼Œæ„Ÿè§‰å¾®å¾®å‡ºæ°´åå°±å¯ï¼Œç”¨ç›–å­è…Œåˆ¶ä¸€æ™šä¸Š\nç¬¬äºŒå¤©è£…è¿›å¯†å°ç›’é‡Œæ”¾å†°ç®±è…Œåˆ¶ä¸€å‘¨å·¦å³ã€‚\nç‚’çš„æ—¶å€™æ‹¿å‡ºæ¥æ¸…æ´—å†æµ¸æ³¡åŠä¸ªå°æ—¶"
}
```

<details><summary>æ•°æ®é›†å¤„ç†è¿‡ç¨‹</summary>

> ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°å¹¶è¿›è¡Œè§£å‹
>
> å°†æ•°æ®è½¬ä¸ºXtunerçš„æ•°æ®æ ¼å¼
>
> åŸæ•°æ®é›†æ–‡ä»¶æ ¼å¼ï¼š
> 
> ```test
>{"name": "æ°´æœæœå†»ï½œå¥åº·å«ç”Ÿ", "dish": "Unknown", "description": "ç»µè½¯çš„æˆšé£è›‹ç³•åº•ï¼Œæ­é…ä¸ŠQå¼¹æ»‘æºœçš„èŠå£«æ…•æ–¯ï¼Œé¡¶å±‚çš„ç™½å·§å…‹åŠ›æ·‹é¢é…±å¥¶é¦™æµ“éƒã€‚\n\nä¸‰ç§æ»‹å‘³åœ¨å£ä¸­äº¤èï¼Œç³Ÿç³•ï¼Œæ˜¯æ‹çˆ±çš„æ„Ÿè§‰ã€‚", "recipeIngredient": ["60gæ°´", "150gé›ªç¢§", "10gå‰åˆ©ä¸ç‰‡", "20gç»†ç ‚ç³–", "3gæŸ æª¬æ±", "è¥¿ç“œ", "å¥‡å¼‚æœ"], "recipeInstructions": ["æ°´+ç»†ç ‚ç³–å€’é”…é‡Œï¼Œå°ç«åŠ çƒ­è‡³ç³–èåŒ–ï¼Œå…³ç«", "å‰åˆ©ä¸ç‰‡ç”¨å†·æ°´æ³¡è½¯åï¼Œæ”¾å…¥åŠ çƒ­å¥½çš„ç³–æ°´ä¸­ï¼Œæ…æ‹Œè‡³å®Œå…¨èåŒ–", "è¶æ¸©çƒ­ï¼Œåœ¨æ··åˆå¥½çš„å‰åˆ©ä¸æ¶²ä½“ä¸­åŠ å…¥æŸ æª¬æ±ï¼Œæ··åˆå‡åŒ€", "å¾…æ¶²ä½“å†·å´åï¼ŒåŠ å…¥é›ªç¢§æ··åˆï¼Œè¿‡æ»¤åæˆæœå†»æ¶²", "æ°´æœæ´—å‡€åï¼Œåˆ‡æˆæƒ³è¦çš„å½¢çŠ¶", "å°†æ°´æœæ”¾å…¥å®¹å™¨ä¸­ï¼Œå€’å…¥æœå†»æ¶²ï¼Œå…¥å†°ç®±å†·å†»è‡³å‡ç»“", "å°†å‡ç»“çš„æœå†»å–å‡ºï¼Œç”¨çƒ­æ¯›å·¾åœ¨å®¹å™¨å¤–æ•·10ç§’å·¦å³ï¼Œå°±å¯ä»¥è„±æ¨¡äº«ç”¨å•¦"], "author": "author_15776", "keywords": ["æ°´æœæœå†»ï½œå¥åº·å«ç”Ÿçš„åšæ³•", "æ°´æœæœå†»ï½œå¥åº·å«ç”Ÿçš„å®¶å¸¸åšæ³•", "æ°´æœæœå†»ï½œå¥åº·å«ç”Ÿçš„è¯¦ç»†åšæ³•", "æ°´æœæœå†»ï½œå¥åº·å«ç”Ÿæ€ä¹ˆåš", "æ°´æœæœå†»ï½œå¥åº·å«ç”Ÿçš„æœ€æ­£å®—åšæ³•", "ç”œå“", "è›‹ç³•", "æˆšé£è›‹ç³•"]}
> ```
>
> ç›®æ ‡æ ¼å¼ï¼š
>
> ```test
> 
> ```
> 

</details>

## å¾®è°ƒ

&emsp;&emsp;ä½¿ç”¨ XTuner è®­ç»ƒï¼Œ XTuner æœ‰å„ä¸ªæ¨¡å‹çš„ä¸€é”®è®­ç»ƒè„šæœ¬ï¼Œå¾ˆæ–¹ä¾¿ã€‚ä¸”å¯¹ InternLM2 çš„æ”¯æŒåº¦æœ€é«˜ã€‚

### XTuner

&emsp;&emsp;ä½¿ç”¨ XTuner è¿›è¡Œå¾®è°ƒï¼Œå…·ä½“è„šæœ¬å¯å‚è€ƒ`configs`æ–‡ä»¶å¤¹ä¸‹çš„è„šæœ¬ï¼Œè„šæœ¬å†…æœ‰è¾ƒä¸ºè¯¦ç»†çš„æ³¨é‡Šã€‚

|åŸºåº§æ¨¡å‹|é…ç½®æ–‡ä»¶|
|:---:|:---:|
|internlm-chat-7b|[internlm_chat_7b_qlora_e3_chineseMed.py](configs/internlm_chat_7b_qlora_e3_chineseMed.py)|
|internlm2-chat-7b|[internlm2_chat_7b_qlora_e3_chineseMed.py](configs/internlm2_chat_7b_qlora_e3_chineseMed.py)|

<details><summary>å¾®è°ƒæ–¹æ³•å¦‚ä¸‹ï¼š</summary>

1. æ ¹æ®åŸºåº§æ¨¡å‹å¤åˆ¶ä¸Šé¢çš„é…ç½®æ–‡ä»¶ï¼Œå°†æ¨¡å‹åœ°å€`pretrained_model_name_or_path`å’Œæ•°æ®é›†åœ°å€`data_path`ä¿®æ”¹æˆè‡ªå·±çš„ï¼Œpropmtæ¨¡æ¿`prompt_template`éœ€è¦æ ¹æ®åŸºåº§æ¨¡å‹æ˜¯InternLMè¿˜æ˜¯InternLM2é€‰æ‹©`PROMPT_TEMPLATE.internlm_chat`è¿˜æ˜¯`PROMPT_TEMPLATE.internlm2_chat`ï¼Œå…¶ä»–å‚æ•°æ ¹æ®è‡ªå·±çš„éœ€æ±‚ä¿®æ”¹ï¼Œç„¶åå°±å¯ä»¥å¼€å§‹å¾®è°ƒï¼ˆå¾®è°ƒæ—¶é—´é•¿çš„æ¨èä½¿ç”¨tmuxï¼Œå…å¾—ä¸‡ä¸€å’Œæœºå™¨æ–­å¼€è¿æ¥å¯¼è‡´å¾®è°ƒä¸­æ–­ï¼‰

   ```bash
   xtuner train ${YOUR_CONFIG} --deepspeed deepspeed_zero2
   ```

   `--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ç­‰ã€‚å¦‚æœç”¨æˆ·æœŸæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œè¯·ç›´æ¥ç§»é™¤æ­¤å‚æ•°ã€‚

2. å°†ä¿å­˜çš„ `.pth` æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨çš„DeepSpeedï¼Œåˆ™å°†ä¼šæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰è½¬æ¢ä¸º HuggingFace Adapter æ¨¡å‹ï¼Œå³ï¼šç”Ÿæˆ Adapter æ–‡ä»¶å¤¹ï¼š

   ```bash
   export MKL_SERVICE_FORCE_INTEL=1
   xtuner convert pth_to_hf ${YOUR_CONFIG} ${PTH} ${LoRA_PATH}
   ```

3. å°† HuggingFace Adapter æ¨¡å‹åˆå¹¶å…¥ HuggingFace æ¨¡å‹ï¼š

    ```bash
    xtuner convert merge ${Base_PATH} ${LoRA_PATH} ${MERGED_PATH}
    ```

4. è‹¥çœŸçš„å‡ºç°æ„å¤–å¯¼è‡´å¾®è°ƒä¸­æ®µï¼Œå¯ä»¥ä»æœ€è¿‘çš„ checkpoint ç»§ç»­å¾®è°ƒ

   ```bash
   xtuner train ${YOUR_CONFIG} --deepspeed deepspeed_zero2 --resume ${LATEST_CHECKPOINT}
   ```

</details>

### Chat

å¾®è°ƒç»“æŸåå¯ä»¥ä½¿ç”¨xtuneræŸ¥çœ‹å¯¹è¯æ•ˆæœ

```shell
xtuner chat ${MERGED_PATH} [optional arguments]
```

<details><summary>å‚æ•°ï¼š</summary>
    
- `--prompt-template`: æŒ‡å®šå¯¹è¯æ¨¡æ¿ï¼Œä¸€ä»£æ¨¡å‹ä½¿ç”¨ internlm_chatï¼ŒäºŒä»£ä½¿ç”¨  internlm2_chatã€‚
- `--system`:  æŒ‡å®šSYSTEMæ–‡æœ¬
- `--system-template`:  æŒ‡å®šSYSTEMæ¨¡æ¿
- `--bits`:  LLMä½æ•°ï¼Œ{4,8,None}ã€‚é»˜è®¤ä¸º fp16ã€‚
- `--bot-name`:  botåç§°
- `--with-plugins`:  æŒ‡å®šè¦ä½¿ç”¨çš„æ’ä»¶
- `--no-streamer`:  æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“
- `--lagent`:  æ˜¯å¦ä½¿ç”¨lagent
- `--command-stop-word`:  å‘½ä»¤åœæ­¢è¯
- `--answer-stop-word`:  å›ç­”åœæ­¢è¯
- `--offload-folder`:  å­˜æ”¾æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼ˆæˆ–è€…å·²ç»å¸è½½æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼‰
- `--max-new-tokens`:  ç”Ÿæˆæ–‡æœ¬ä¸­å…è®¸çš„æœ€å¤§ token æ•°é‡
- `--temperature`:  æ¸©åº¦å€¼ï¼Œå¯¹äºäºŒä»£æ¨¡å‹ï¼Œå»ºè®®ä¸º0.8ã€‚
- `--top-k`:  ä¿ç•™ç”¨äºé¡¶kç­›é€‰çš„æœ€é«˜æ¦‚ç‡è¯æ±‡æ ‡è®°æ•°
- `--top-p`:  å¦‚æœè®¾ç½®ä¸ºå°äº1çš„æµ®ç‚¹æ•°ï¼Œä»…ä¿ç•™æ¦‚ç‡ç›¸åŠ é«˜äº top_p çš„æœ€å°ä¸€ç»„æœ€æœ‰å¯èƒ½çš„æ ‡è®°ï¼Œå¯¹äºäºŒä»£æ¨¡å‹ï¼Œå»ºè®®ä¸º0.8ã€‚
- `--repetition-penalty`: é˜²æ­¢æ–‡æœ¬é‡å¤è¾“å‡ºï¼Œå¯¹äºäºŒä»£æ¨¡å‹ï¼Œä¸ªäººå»ºè®®1.01ï¼Œå¯¹äºä¸€ä»£æ¨¡å‹å¯ä¸å¡«ã€‚
- `--seed`:  ç”¨äºå¯é‡ç°æ–‡æœ¬ç”Ÿæˆçš„éšæœºç§å­
- `-h`:  æŸ¥çœ‹å‚æ•°ã€‚
  
</details>

## OpenXLab éƒ¨ç½² ä¸­åŒ»è¯çŸ¥è¯†é—®ç­”åŠ©æ‰‹

&emsp;&emsp;ä»…éœ€è¦ Fork [æ­¤ä»“åº“](https://github.com/xiaomile/medKnowledgeAssitant)ï¼Œç„¶ååœ¨ OpenXLab ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„é¡¹ç›®ï¼Œå°† Fork çš„ä»“åº“ä¸æ–°å»ºçš„é¡¹ç›®å…³è”ï¼Œå³å¯åœ¨ OpenXLab ä¸Šéƒ¨ç½² ä¸­åŒ»è¯çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚

&emsp;&emsp;***OPenXLab ä¸­åŒ»è¯çŸ¥è¯†é—®ç­”åŠ©æ‰‹  https://openxlab.org.cn/apps/detail/xiaomile/medKnowledgeAssitant***
&emsp;&emsp;***OPenXLab ä¸­åŒ»è¯çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼ˆ4bitï¼‰  https://openxlab.org.cn/apps/detail/xiaomile/personal_assistant2_4bit***

![Alt text](images/openxlab.png)
![4bit](images/openxlab2.png)

## LmDeployéƒ¨ç½²

- é¦–å…ˆå®‰è£…LmDeploy

```shell
pip install -U 'lmdeploy[all]==v0.2.0'
```

- ç„¶åè½¬æ¢æ¨¡å‹ä¸º`turbomind`æ ¼å¼ã€‚ä½¿ç”¨ TurboMind æ¨ç†æ¨¡å‹éœ€è¦å…ˆå°†æ¨¡å‹è½¬åŒ–ä¸º TurboMind çš„æ ¼å¼ï¼Œï¼Œç›®å‰æ”¯æŒåœ¨çº¿è½¬æ¢å’Œç¦»çº¿è½¬æ¢ä¸¤ç§å½¢å¼ã€‚TurboMind æ˜¯ä¸€æ¬¾å…³äº LLM æ¨ç†çš„é«˜æ•ˆæ¨ç†å¼•æ“ï¼ŒåŸºäºè‹±ä¼Ÿè¾¾çš„ FasterTransformer ç ”å‘è€Œæˆã€‚å®ƒçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼šLLaMa ç»“æ„æ¨¡å‹çš„æ”¯æŒï¼Œpersistent batch æ¨ç†æ¨¡å¼å’Œå¯æ‰©å±•çš„ KV ç¼“å­˜ç®¡ç†å™¨ã€‚
æœ¬é¡¹ç›®é‡‡ç”¨ç¦»çº¿è½¬æ¢ï¼Œéœ€è¦åœ¨å¯åŠ¨æœåŠ¡ä¹‹å‰ï¼Œå°†æ¨¡å‹è½¬ä¸º lmdeploy TurboMind çš„æ ¼å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

> --dst-path: å¯ä»¥æŒ‡å®šè½¬æ¢åçš„æ¨¡å‹å­˜å‚¨ä½ç½®ã€‚

```shell
lmdeploy convert internlm-chat-7b  è¦è½¬åŒ–çš„æ¨¡å‹åœ°å€ --dst-path ./workspace è½¬æ¢åæ¨¡å‹çš„å­˜æ”¾åœ°å€
```
æ‰§è¡Œå®Œæˆåå°†ä¼šåœ¨å½“å‰ç›®å½•ç”Ÿæˆä¸€ä¸ª workspace çš„æ–‡ä»¶å¤¹ã€‚

- LmDeploy Chatå¯¹è¯ã€‚æ¨¡å‹è½¬æ¢å®Œæˆåï¼Œæˆ‘ä»¬å°±å…·å¤‡äº†ä½¿ç”¨æ¨¡å‹æ¨ç†çš„æ¡ä»¶ï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥è¿›è¡ŒçœŸæ­£çš„æ¨¡å‹æ¨ç†ç¯èŠ‚ã€‚
1ã€æœ¬åœ°å¯¹è¯ï¼ˆBash Local Chatï¼‰æ¨¡å¼ï¼Œå®ƒæ˜¯è·³è¿‡API Serverç›´æ¥è°ƒç”¨TurboMindã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯å‘½ä»¤è¡Œä»£ç ç›´æ¥æ‰§è¡Œ TurboMindã€‚
```shell
lmdeploy chat turbomind ./workspace #è½¬æ¢åçš„turbomindæ¨¡å‹åœ°å€
```

- ç½‘é¡µDemoæ¼”ç¤ºã€‚æœ¬é¡¹ç›®é‡‡ç”¨å°†TurboMindæ¨ç†ä½œä¸ºåç«¯ï¼Œå°†Gradioä½œä¸ºå‰ç«¯Demoæ¼”ç¤ºã€‚
```shell
# Gradio+Turbomind(local)
lmdeploy serve gradio ./workspace #è½¬æ¢åçš„turbomindæ¨¡å‹åœ°å€
```
å°±å¯ä»¥ç›´æ¥å¯åŠ¨ Gradioï¼Œæ­¤æ—¶æ²¡æœ‰API Serverï¼ŒTurboMindç›´æ¥ä¸Gradioé€šä¿¡ã€‚
- åŸå§‹æ¨¡å‹è¿è¡Œï¼Œæ˜¾å­˜å ç”¨56%

## Lmdeploy&opencompass é‡åŒ–ä»¥åŠé‡åŒ–è¯„æµ‹  
> è¿›è¡Œé‡åŒ–å†³ç­–æµç¨‹
> Step1:å°è¯•æ­£å¸¸ç‰ˆæœ¬ï¼Œè¯„ä¼°æ•ˆæœã€‚æ•ˆæœä¸€èˆ¬ï¼Œå¯åŠ¨é‡åŒ–ã€‚
> Step2:å¼€å±•KV Cacheé‡åŒ–ï¼Œä»¥å‡å°‘ä¸­é—´è¿‡ç¨‹è®¡ç®—ç»“æœå¯¹æ˜¾å­˜çš„å ç”¨ã€‚è¯„ä¼°é‡åŒ–æ•ˆæœã€‚
### `KV Cache`é‡åŒ– 
- è®¡ç®—ä¸è·å¾—é‡åŒ–å‚æ•°
  >è®¡ç®— minmaxã€‚ä¸»è¦æ€è·¯æ˜¯é€šè¿‡è®¡ç®—ç»™å®šè¾“å…¥æ ·æœ¬åœ¨æ¯ä¸€å±‚ä¸åŒä½ç½®å¤„è®¡ç®—ç»“æœçš„ç»Ÿè®¡æƒ…å†µã€‚
  >åœ¨è®¡ç®—minmaxçš„å‘½ä»¤è¡Œä¸­ï¼Œä¼šé€‰æ‹©128æ¡è¾“å…¥æ ·æœ¬ï¼Œæ¯æ¡æ ·æœ¬é•¿åº¦ä¸º 2048ï¼Œæ•°æ®é›†é€‰æ‹©ptbï¼Œè¾“å…¥æ¨¡å‹åå°±ä¼šå¾—åˆ°ä¸Šé¢çš„å„ç§ç»Ÿè®¡å€¼ã€‚
```shell
# è®¡ç®— minmax
lmdeploy lite calibrate \
  ./internlm-chat-7b/  #æ¨¡å‹ç»å¯¹è·¯å¾„ \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 2048 \
  --work-dir ./quant_output #å‚æ•°ä¿å­˜è·¯å¾„
  --trust_remote_code=True
```
  >é€šè¿‡minmaxè·å–é‡åŒ–å‚æ•°ã€‚ä¸»è¦åˆ©ç”¨ä¸‹é¢å…¬å¼æ¥è·å–æ¯ä¸€å±‚çš„KVä¸­å¿ƒå€¼ï¼ˆzpï¼‰å’Œç¼©æ”¾å€¼ï¼ˆscaleï¼‰ã€‚
```shell
zp = (min+max) / 2
scale = (max-min) / 255
quant: q = round( (f-zp) / scale)
dequant: f = q * scale + zp
```
  >æœ‰äº†è¿™ä¸¤ä¸ªå€¼å°±å¯ä»¥è¿›è¡Œé‡åŒ–å’Œåé‡åŒ–æ“ä½œã€‚å…·ä½“æ¥è¯´å°±æ˜¯å¯¹å†å²å­˜å‚¨ä¸­çš„Kå’ŒVåšé‡åŒ–ï¼Œä½¿ç”¨æ—¶å†åé‡åŒ–ã€‚ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š
```shell
# é€šè¿‡ minmax è·å–é‡åŒ–å‚æ•°
lmdeploy lite kv_qparams \
   ./quant_output #ä¿å­˜kvè®¡ç®—ç»“æœçš„è·¯å¾„ \
   workspace/triton_models/weights/ #è½¬æ¢åæ¨¡å‹çš„å­˜æ”¾è·¯å¾„ \
  --num_tp 1
```
- ä¿®æ”¹é…ç½®ã€‚ä¿®æ”¹weights/config.iniæ–‡ä»¶ï¼ŒæŠŠquant_policyæ”¹ä¸º4ï¼Œä»è€Œæ‰“å¼€KV int8å¼€å…³ã€‚
```shell
tensor_para_size = 1
session_len = 2056
max_batch_size = 64
max_context_token_num = 1
step_length = 1
cache_max_entry_count = 0.5
cache_block_seq_len = 128
cache_chunk_size = 1
use_context_fmha = 1
quant_policy = 4
max_position_embeddings = 2048
rope_scaling_factor = 0.0
use_logn_attn = 0
```
  >è‡³æ­¤å°±å®Œæˆäº†KV Cacheé‡åŒ–ã€‚
  >å¼€å§‹å¯¹è¯
```shell
lmdeploy chat turbomind /root/chinesemedical/workspace --model-format hf  --quant-policy 4
```

- è¯„ä¼°é‡åŒ–æ•ˆæœã€‚ç¼–å†™è¯„æµ‹æ–‡ä»¶`configs/eval_turbomind.py`
```python
from mmengine.config import read_base
from opencompass.models.turbomind import TurboMindModel

with read_base():
 # choose a list of datasets   
  from .datasets.ceval.ceval_gen import ceval_datasets 
 # and output the results in a choosen format
  from .summarizers.medium import summarizer

datasets = [*ceval_datasets]

internlm2_chat_7b = dict(
     type=TurboMindModel,
     abbr='internlm2-chat-7b-turbomind',
     path='è½¬æ¢åçš„æ¨¡å‹åœ°å€',
     engine_config=dict(session_len=512,
         max_batch_size=2,
         rope_scaling_factor=1.0),
     gen_config=dict(top_k=1,
         top_p=0.8,
         temperature=1.0,
         max_new_tokens=100),
     max_out_len=100,
     max_seq_len=512,
     batch_size=2,
     concurrency=1,
     #  meta_template=internlm_meta_template,
     run_cfg=dict(num_gpus=1, num_procs=1),
)
models = [internlm2_chat_7b]
```
- å¯åŠ¨è¯„æµ‹ï¼
```shell
python run.py configs/eval_turbomind.py -w æŒ‡å®šç»“æœä¿å­˜è·¯å¾„
```
- å•ç‹¬åšKV Cacheé‡åŒ–ï¼Œæ˜¾å­˜å ç”¨55%ï¼Œæ— æ˜æ˜¾ä¼˜åŒ–ï¼
  
> Step3:å¼€å±•W4A16é‡åŒ–ï¼Œä»¥å‡å°‘æ¨¡å‹å‚æ•°è®¡ç®—ç»“æœå¯¹æ˜¾å­˜çš„å ç”¨ã€‚è¯„ä¼°é‡åŒ–æ•ˆæœã€‚W4A16ä¸­çš„Aæ˜¯æŒ‡Activationï¼Œä¿æŒFP16ï¼Œåªå¯¹éƒ¨åˆ†æƒé‡å‚æ•°è¿›è¡Œ4bité‡åŒ–
### `W4A16`é‡åŒ– 
- è®¡ç®—ä¸è·å¾—é‡åŒ–å‚æ•°
  >è®¡ç®— minmaxã€‚ä¸»è¦æ€è·¯æ˜¯é€šè¿‡è®¡ç®—ç»™å®šè¾“å…¥æ ·æœ¬åœ¨æ¯ä¸€å±‚ä¸åŒä½ç½®å¤„è®¡ç®—ç»“æœçš„ç»Ÿè®¡æƒ…å†µã€‚
  >åœ¨è®¡ç®—minmaxçš„å‘½ä»¤è¡Œä¸­ï¼Œä¼šé€‰æ‹©128æ¡è¾“å…¥æ ·æœ¬ï¼Œæ¯æ¡æ ·æœ¬é•¿åº¦ä¸º 2048ï¼Œæ•°æ®é›†é€‰æ‹©ptbï¼Œè¾“å…¥æ¨¡å‹åå°±ä¼šå¾—åˆ°ä¸Šé¢çš„å„ç§ç»Ÿè®¡å€¼ã€‚
```shell
# è®¡ç®— minmax
lmdeploy lite calibrate \
  ./internlm-chat-7b/  #æ¨¡å‹ç»å¯¹è·¯å¾„ \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 2048 \
  --work-dir ./quant_output #å‚æ•°ä¿å­˜è·¯å¾„
  --trust_remote_code=True
```
- é‡åŒ–æƒé‡æ¨¡å‹
  >åˆ©ç”¨ä¸Šé¢å¾—åˆ°çš„ç»Ÿè®¡å€¼å¯¹å‚æ•°è¿›è¡Œé‡åŒ–ã€‚
  >æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
```shell
# é‡åŒ–æƒé‡æ¨¡å‹
lmdeploy lite auto_awq \
  ./internlm-chat-7b/   #æœªé‡åŒ–å‰æ¨¡å‹çš„å­˜æ”¾è·¯å¾„ \
  --calib-dataset 'ptb' \
  --calib-samples 128 \ 
  --calib-seqlen 2048 \
  --w-bits 4 \
  --w_group_size 128 \
  --work_dir ./internlm-chat-7b-4bit #é‡åŒ–åæ¨¡å‹çš„å­˜æ”¾è·¯å¾„
```
  >å‘½ä»¤ä¸­ w_bitsè¡¨ç¤ºé‡åŒ–çš„ä½æ•°ï¼Œw_group_sizeè¡¨ç¤ºé‡åŒ–åˆ†ç»„ç»Ÿè®¡çš„å°ºå¯¸ï¼Œwork_diræ˜¯é‡åŒ–åæ¨¡å‹è¾“å‡ºçš„ä½ç½®ã€‚
  >å› ä¸ºæ²¡æœ‰ torch.int4ï¼Œæ‰€ä»¥å®é™…å­˜å‚¨æ—¶ï¼Œ8ä¸ª4bitæƒé‡ä¼šè¢«æ‰“åŒ…åˆ°ä¸€ä¸ªint32å€¼ä¸­ã€‚
- è½¬æ¢æˆ TurboMind æ ¼å¼ï¼ˆä¹Ÿå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ï¼Œç›´æ¥å¯åŠ¨å¯¹è¯ï¼‰
```shell
# è½¬æ¢æ¨¡å‹çš„layoutï¼Œå­˜æ”¾åœ¨é»˜è®¤è·¯å¾„ ./workspace ä¸‹
lmdeploy convert  internlm-chat-7b ./internlm-chat-7b-4bit/ #W4A16é‡åŒ–åçš„æ¨¡å‹è·¯å¾„\
    --model-format awq \
    --group-size 128
    --dst_path ./workspace_4bit #è½¬æ¢åæ¨¡å‹çš„å­˜æ”¾è·¯å¾„
``` 
  >è¿™ä¸ªgroup-sizeå°±æ˜¯é‚£ä¸ªw_group_sizeã€‚å¯ä»¥æŒ‡å®šè¾“å‡ºç›®å½•ï¼š--dst_pathã€‚
  >è‡³æ­¤å°±å®Œæˆäº†W4A16é‡åŒ–ã€‚
- å¯åŠ¨å¯¹è¯
```shell
lmdeploy chat turbomind ./workspace_4bit --model-format awq
``` 
- è¯„ä¼°é‡åŒ–æ•ˆæœã€‚è¯„æµ‹æ–‡ä»¶`configs/eval_turbomind.py`å¦‚ä¸Š
- å¯åŠ¨è¯„æµ‹ï¼
```shell
python run.py configs/eval_turbomind.py -w ç»“æœä¿å­˜è·¯å¾„
```
ç»“æœæ–‡ä»¶å¯åœ¨åŒç›®å½•æ–‡ä»¶[results](./results)ä¸­è·å–
- å•ç‹¬åšW4A16é‡åŒ–ï¼Œæ˜¾å­˜å ç”¨64%ï¼Œè¾ƒæœªé‡åŒ–å‰æ¨¡å‹å ç”¨å†…å­˜æ›´å¤§ï¼


> Step4:åŒæ­¥å¼€å¯KV Cacheé‡åŒ–å’ŒW4A16é‡åŒ–ï¼Œä»¥å‡å°‘ä¸­é—´è¿‡ç¨‹è®¡ç®—ç»“æœå’Œæ¨¡å‹å‚æ•°è®¡ç®—ç»“æœå¯¹æ˜¾å­˜çš„å ç”¨ã€‚
- è·å–å¯¹W4A16é‡åŒ–åæ¨¡å‹çš„KV Cacheé‡åŒ–å‚æ•°
```shell
lmdeploy lite kv_qparams \
   ./quant_output/  \  # å­˜æ”¾ä¹‹å‰kv cacheè®¡ç®—ç»“æœçš„æ–‡ä»¶å¤¹è·¯å¾„
  workspace_4bit/triton_models/weights/ \ # å­˜æ”¾æœ¬æ¬¡kv cacheé‡åŒ–åå‚æ•°çš„æ–‡ä»¶å¤¹è·¯å¾„
  --num-tp 1
```
- ä¿®æ”¹å‚æ•°
å¯¹workspace_4bit/triton_models/weights/config.iniæ–‡ä»¶è¿›è¡Œå‚æ•°ä¿®æ”¹
```shell
cache_max_entry_count = 0.2
quant_policy = 4
```
- å¯åŠ¨å¯¹è¯
```shell
lmdeploy chat turbomind ./workspace_4bit/  --model-format awq --quant-policy 4
```
- åŒæ­¥å¼€å¯KV Cacheé‡åŒ–å’ŒW4A16é‡åŒ–ï¼Œæ˜¾å­˜å ç”¨34%ï¼Œæœ‰æ˜æ˜¾ä¼˜åŒ–æ•ˆæœï¼


## OpenCompass è¯„æµ‹

- å®‰è£… OpenCompass

```shell
git clone https://github.com/open-compass/opencompass
cd opencompass
pip install -e .
```

- åœ¨opencompass/configsç›®å½•ä¸‹æ–°å»ºè‡ªå®šä¹‰æ•°æ®é›†æµ‹è¯„é…ç½®æ–‡ä»¶ `eval_internlm_7b_custom.py` å’Œ `eval_internlm_chat_turbomind_api_custom.py`

```python
from mmengine.config import read_base
from opencompass.models import HuggingFaceCausalLM

with read_base():
    from .summarizers.medium import summarizer

datasets = [
    {"path": "/root/ChineseMedicalAssistant/test_qa.jsonl", "data_type": "qa", "infer_method": "gen"}, # your custom dataset
]

internlm_chat_7b = dict(
       type=HuggingFaceCausalLM,
       # `HuggingFaceCausalLM` çš„åˆå§‹åŒ–å‚æ•°
       path='/root/ChineseMedicalAssistant/merged', # your model path
       tokenizer_path='/root/ChineseMedicalAssistant/merged', # your model path
       tokenizer_kwargs=dict(
           padding_side='left',
           truncation_side='left',
           proxies=None,
           trust_remote_code=True),
       model_kwargs=dict(device_map='auto',trust_remote_code=True),
       # ä¸‹é¢æ˜¯æ‰€æœ‰æ¨¡å‹çš„å…±åŒå‚æ•°ï¼Œä¸ç‰¹å®šäº HuggingFaceCausalLM
       abbr='internlm_chat_7b',               # ç»“æœæ˜¾ç¤ºçš„æ¨¡å‹ç¼©å†™
       max_seq_len=2048,             # æ•´ä¸ªåºåˆ—çš„æœ€å¤§é•¿åº¦
       max_out_len=100,              # ç”Ÿæˆçš„æœ€å¤§ token æ•°
       batch_size=64,                # æ‰¹é‡å¤§å°
       run_cfg=dict(num_gpus=1),     # è¯¥æ¨¡å‹æ‰€éœ€çš„ GPU æ•°é‡
    )

models=[internlm_chat_7b]
```

```python
from mmengine.config import read_base
from opencompass.models.turbomind_api import TurboMindAPIModel

with read_base():
    from .summarizers.medium import summarizer

datasets = [
    {"path": "/root/ChineseMedicalAssistant/test_qa.jsonl", "data_type": "qa", "infer_method": "gen"}, # your custom dataset
]


meta_template = dict(
    round=[
        dict(role='HUMAN', begin='<|User|>:', end='\n'),
        dict(role='BOT', begin='<|Bot|>:', end='<eoa>\n', generate=True),
    ],
    eos_token_id=103028)

models = [
    dict(
        type=TurboMindAPIModel,
        abbr='internlm-chat-7b-turbomind',
        path="./model/workspace_4bit",
        api_addr='http://0.0.0.0:23333',
        max_out_len=100,
        max_seq_len=2048,
        batch_size=8,
        meta_template=meta_template,
        run_cfg=dict(num_gpus=1, num_procs=1),
    )
]
```

- è¯„æµ‹å¯åŠ¨ï¼

```shell
python run.py configs/eval_internlm_7b_custom.py
```

- é‡åŒ–è¯„æµ‹ï¼Œå…ˆå¯åŠ¨turbomindä½œä¸ºæœåŠ¡ç«¯

```shell
lmdeploy serve api_server ./workspace_4bit --server_name 0.0.0.0 --server_port 23333 --instance_num 64 --tp 1
```

```shell
python run.py eval_internlm_chat_turbomind_api_custom.py
```


## è‡´è°¢

<div align="center">

***æ„Ÿè°¢ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ç»„ç»‡çš„ ä¹¦ç”ŸÂ·æµ¦è¯­å®æˆ˜è¥ å­¦ä¹ æ´»åŠ¨~***

***æ„Ÿè°¢ OpenXLab å¯¹é¡¹ç›®éƒ¨ç½²çš„ç®—åŠ›æ”¯æŒ~***

***æ„Ÿè°¢ æµ¦è¯­å°åŠ©æ‰‹ å¯¹é¡¹ç›®çš„æ”¯æŒ~***
</div>
